# -*- coding: utf-8 -*-
"""time_series_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bn5I9gBtE9cyyl6ywHrEK_xD5kNuuXbG
"""

import math # Mathematical functions 
import numpy as np # Fundamental package for scientific computing with Python
import pandas as pd # Additional functions for analysing and manipulating data
from datetime import date, timedelta, datetime # Date Functions
from pandas.plotting import register_matplotlib_converters # This function adds plotting functions for calender dates
import matplotlib.pyplot as plt # Important package for visualization - we use this to plot the market data
import matplotlib.dates as mdates # Formatting dates
from sklearn.metrics import mean_absolute_error, mean_squared_error # Packages for measuring model performance / errors
from keras.models import Sequential, load_model # Deep learning library, used for neural networks
from keras.layers import LSTM, Dense, Dropout # Deep learning classes for recurrent and regular densely-connected layers
from keras.callbacks import EarlyStopping # EarlyStopping during model training
import seaborn as sns
from sklearn import preprocessing
import calendar
from sklearn.model_selection import TimeSeriesSplit

data = pd.read_csv("/content/drive/MyDrive/temp/train.csv")

data.head()

# check columns with data type object
col_list = [c for c in data.columns if data[c].dtype == 'object' and c != 'timestamp']
print(col_list)

#Checking null are available in dataframe columns
data.isnull().sum()
data.groupby(by=['Month of Sourcing']).count()

#pre-processing the data.
le = preprocessing.LabelEncoder()
data['ProductType']  = le.fit_transform(data['ProductType'])
data['Manufacturer']  = le.fit_transform(data['Manufacturer'])
data['Area Code']  = le.fit_transform(data['Area Code'])
data['Sourcing Channel']  = le.fit_transform(data['Sourcing Channel'])
data['Product Size']  = le.fit_transform(data['Product Size'])
data['Product Type']  = le.fit_transform(data['Product Type'])

data['Month of Sourcing'] = pd.to_datetime([f'20{y}-{m}-01' for y, m in zip(data['Month of Sourcing'].str.split('-').str[1], data['Month of Sourcing'].str.split('-').str[0])])

data = data.sort_values(by='Month of Sourcing', ascending=False)
data

data = data.set_index('Month of Sourcing')

data

# Plot line charts
df_plot = data.copy()

list_length = df_plot.shape[1]
ncols = 2
nrows = int(round(list_length / ncols, 0))

fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(14, 7))
fig.subplots_adjust(hspace=0.5, wspace=0.5)
for i in range(0, list_length):
        ax = plt.subplot(nrows,ncols,i+1)
        sns.lineplot(data = df_plot.iloc[:, i], ax=ax)
        ax.set_title(df_plot.columns[i])
        ax.tick_params(axis="x", rotation=30, labelsize=10, length=0)
        ax.xaxis.set_major_locator(mdates.AutoDateLocator())
fig.tight_layout()
plt.show()

train_size = int(len(data) * 0.8)
test_size = len(data) - train_size
train, test = data.iloc[0:train_size], data.iloc[train_size:len(data)]
print(len(train), len(test))

cols = [col for col in data.columns if 'Sourcing Cost' not in col]
f_transformer = preprocessing.RobustScaler()
f_transformer = f_transformer.fit(train[cols].to_numpy())
train.loc[:, cols] = f_transformer.transform(
  train[cols].to_numpy()
)
test.loc[:, cols] = f_transformer.transform(
  test[cols].to_numpy()
)

cost_transformer = preprocessing.RobustScaler()
cnt_transformer = cost_transformer.fit(train[['Sourcing Cost']])
train['Sourcing Cost'] = cnt_transformer.transform(train[['Sourcing Cost']])
test['Sourcing Cost'] = cnt_transformer.transform(test[['Sourcing Cost']])

def create_dataset(X, y, time_steps=1):
    Xs, ys = [], []
    for i in range(len(X) - time_steps):
        # print(i, time_steps)
        Xs.append(X.iloc[i:(i + time_steps)].values)
        ys.append(y.iloc[i + time_steps])
    return np.array(Xs), np.array(ys)

range(len(test_data))

time_steps = 100
# reshape to [samples, time_steps, n_features]
X_train, y_train = create_dataset(train, train['Sourcing Cost'], time_steps)
X_test, y_test = create_dataset(test, test['Sourcing Cost'], time_steps)
print(X_train.shape, y_train.shape)

data

"""## **Model training**"""

# Configure the neural network model
model = Sequential()

# Model with n_neurons = inputshape Timestamps, each with x_train.shape[2] variables
n_neurons = X_train.shape[1] * X_train.shape[2]
print(n_neurons, X_train.shape[1])
model.add(LSTM(n_neurons, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))) 
model.add(LSTM(n_neurons, return_sequences=False))
model.add(Dropout(rate=0.2))
model.add(Dense(5))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mse')

model.summary()

# Training the model
# we can try better epochs and batch_size using keras tunner.
epochs = 10
batch_size = 3000
early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)
history = model.fit(X_train, y_train, 
                    batch_size=batch_size, 
                    epochs=epochs,
                    validation_data=(X_test, y_test)
                   )

model.save('/content/drive/MyDrive/temp/time_series_model.h5')

test_data = pd.read_csv('/content/drive/MyDrive/temp/test.csv')

#pre-processing the data.
le = preprocessing.LabelEncoder()
test_data['ProductType']  = le.fit_transform(test_data['ProductType'])
test_data['Manufacturer']  = le.fit_transform(test_data['Manufacturer'])
test_data['Area Code']  = le.fit_transform(test_data['Area Code'])
test_data['Sourcing Channel']  = le.fit_transform(test_data['Sourcing Channel'])
test_data['Product Size']  = le.fit_transform(test_data['Product Size'])
test_data['Product Type']  = le.fit_transform(test_data['Product Type'])
test_data['Month of Sourcing'] = pd.to_datetime([f'20{y}-{m}-01' for y, m in zip(test_data['Month of Sourcing'].str.split('-').str[1], test_data['Month of Sourcing'].str.split('-').str[0])])
test_data = test_data.set_index('Month of Sourcing')

cols = [col for col in test_data.columns if 'Sourcing Cost' not in col]
f_transformer = preprocessing.RobustScaler()
f_transformer = f_transformer.fit(test_data[cols].to_numpy())
test_data.loc[:, cols] = f_transformer.transform(
  test_data[cols].to_numpy()
)

test_data.shape

X_test_data, y_test_data = create_dataset(test_data, test_data['Sourcing Cost'],)

X_test_data.shape

model = load_model('/content/drive/MyDrive/temp/time_series_model.h5')

forecast = model.predict(X_test_data) #forecast 
print(forecast.shape)
#Perform inverse transformation to rescale back to original range
#Since we used 5 variables for transform, the inverse expects same dimensions
#Therefore, let us copy our values 5 times and discard them after inverse transform
forecast_copies = np.repeat(forecast, X_test_data.shape[1], axis=-1)
# forecast_copies = forecast_copies.reshape(X_test_data.shape[1])
# print(forecast_copies)
y_pred_future = cost_transformer.inverse_transform(forecast_copies)

# Plot training & validation loss values
fig, ax = plt.subplots(figsize=(20, 10), sharex=True)
plt.plot(history.history["loss"])
plt.title("Model loss")
plt.ylabel("Loss")
plt.xlabel("Epoch")
ax.xaxis.set_major_locator(plt.MaxNLocator(epochs))
plt.legend(["Train", "Test"], loc="upper left")
plt.grid()
plt.show()

"""# Evaluate Model Performance"""

y_test_data = test_data['Sourcing Cost'].iloc[0:y_pred_future.shape[0]]
y_test_data.shape
y_pred_future = y_pred_future.reshape(y_pred_future.shape[0])

# Mean Absolute Error (MAE)
MAE = mean_absolute_error(y_test_data, y_pred_future)
print(f'Median Absolute Error (MAE): {np.round(MAE, 2)}')

# Mean Absolute Percentage Error (MAPE)
MAPE = np.mean((np.abs(np.subtract(y_test_data, y_pred_future)/ y_test_data))) * 100
print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')

# Median Absolute Percentage Error (MDAPE)
MDAPE = np.median((np.abs(np.subtract(y_test_data, y_pred_future)/ y_test_data)) ) * 100
print(f'Median Absolute Percentage Error (MDAPE): {np.round(MDAPE, 2)} %')

"""### As I have tried both machine learning and deep learning models. we can try to hypermatter for deep learning model to increase the accuracy. For now  machine learning algorithm given best accuracy so selecting machine learning algorithm."""